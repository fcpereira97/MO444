{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules import\n",
    "\n",
    "import random\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class of colors to use with the print funtion\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class that represents a sample\n",
    "\n",
    "class Sample:\n",
    "    def __init__(self, identifier, features):\n",
    "        self.identifier = identifier #Number that identifies the sample\n",
    "        self.features = features #Array of feature values of the sample\n",
    "        self.cluster = None\n",
    "        self.silhouette_a = None\n",
    "        self.silhouette_b = None\n",
    "        self.silhouette_score = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class that represents a centroid\n",
    "\n",
    "class Centroid:\n",
    "    def __init__(self, identifier, features):\n",
    "        self.identifier = identifier #Number that identifies the centroid\n",
    "        self.features = features #Array of feature values of the centroid\n",
    "        self.list_of_samples = [] #Array containing all the samples that are assigned to this centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class that represents a cluster\n",
    "class Cluster:\n",
    "    def __init__(self, identifier, list_of_samples):\n",
    "        self.identifier = identifier #Number that identifies the cluster\n",
    "        self.list_of_samples = list_of_samples #Array containing all the samples that are assigned to this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that applies the z-score method of normalization and standardization\n",
    "\n",
    "def z_score_normalization(dataset, n_dimensions):\n",
    "    for i in range(n_dimensions): #For each dimension i\n",
    "        feature_values = [] #Create a list of all feature values of the i-th dimension\n",
    "        for sample in dataset: #Fill the list\n",
    "            feature_values.append(sample.features[i])\n",
    "        feature_average = statistics.mean(feature_values) #Get the average value\n",
    "        feature_stdev = statistics.stdev(feature_values) #Get the standard deviation value\n",
    "        for sample in dataset: #For each sample, adjust its feature value of the i-th dimension\n",
    "            sample.features[i] = (sample.features[i] - feature_average)/(feature_stdev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that applies the min-max method of normalization\n",
    "\n",
    "def min_max_normalization(dataset, n_dimensions):\n",
    "    for i in range(n_dimensions): #For each dimension i\n",
    "        max_value = -math.inf #Maximum value between the feature values of the i-th dimension\n",
    "        min_value = math.inf #Minimum value between the feature values of the i-th dimension\n",
    "        for sample in dataset: #Calculate max_value and min_value\n",
    "            if sample.features[i] > max_value:\n",
    "                max_value = sample.features[i]\n",
    "            if sample.features[i] < min_value:\n",
    "                min_value = sample.features[i]\n",
    "        for sample in dataset: #For each sample, adjust its feature value of the i-th dimension\n",
    "            sample.features[i] = (sample.features[i] - min_value)/(max_value - min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the dataset from input file\n",
    "\n",
    "def get_dataset(filename):\n",
    "\n",
    "    dataset_file = open(filename,\"r\") #Get the input file\n",
    "    dataset_file_lines = dataset_file.readlines() #Read all the lines of the input file\n",
    "    dataset = [] #Array that will contain all samples objects\n",
    "    \n",
    "    for i in range(len(dataset_file_lines)): #For each line of the input file\n",
    "        dataset_file_lines[i] = dataset_file_lines[i].split() #Split the line at the spaces\n",
    "        features = list(map(float, dataset_file_lines[i])) #Get the features values list in float format\n",
    "        new_sample = Sample(i+1, features) #Create new Sample object\n",
    "        dataset.append(new_sample) #Append the new sample to the dataset\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataset into training and test sets\n",
    "\n",
    "def divide_dataset(dataset, training_set_proportion, test_set_proportion):\n",
    "\n",
    "    training_set_size = math.floor(training_set_proportion*len(dataset)) #Get the size of the training set\n",
    "    #test_set_size = math.ceil(test_set_proportion*len(dataset)) #Get the size of the test set\n",
    "    training_set = [] #Traning set array\n",
    "    test_set = [] #Test set array\n",
    "\n",
    "    for i in range(len(dataset)): #Partition the dataset into traning and test sets\n",
    "        if i < training_set_size:\n",
    "            training_set.append(dataset[i])\n",
    "        else:\n",
    "            test_set.append(dataset[i])\n",
    "    return training_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that calculates the euclidean distance between two samples\n",
    "\n",
    "def get_distance(sample1, sample2, n_dimensions):\n",
    "    sum = 0\n",
    "    for i in range(n_dimensions):\n",
    "        sum = sum + (sample1.features[i] - sample2.features[i])**2\n",
    "    return math.sqrt(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that plots the elbow graph with repetitions for a each value of k\n",
    "\n",
    "def plot_elbow_graph(dataset, n_dimensions, min_k, max_k, n_repetitions):\n",
    "    \n",
    "    elbow_points = [] #Points of the elbow graph\n",
    "    for k in range (min_k, max_k + 1): #For each value of k between min_k and max_k\n",
    "        average = 0 #Variable that will contain the SSE average value \n",
    "        for j in range(n_repetitions): #Execute k-means n_repetitions times with same value of k\n",
    "            clusters, centroids, sse = kmeans(dataset, k, n_dimensions) #Run the k-means algorithm\n",
    "            average += sse\n",
    "        average = float(average)/n_repetitions \n",
    "        elbow_points.append([k,average]) #Add the point (k, SSE average value)\n",
    "\n",
    "    #Plot the graph\n",
    "    x = []\n",
    "    y = []\n",
    "    for point in elbow_points:\n",
    "        x.append(point[0])\n",
    "        y.append(point[1])\n",
    "\n",
    "    plt.xlabel(\"K\", fontsize=15)\n",
    "    plt.ylabel(\"SSE\", fontsize=15)\n",
    "    plt.xticks(list(range(min_k,max_k + 1)))    \n",
    "    plt.grid(linestyle='--')\n",
    "    plt.plot(x,y,'o-')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuction that calculates the silhouette score of a clusterization\n",
    "\n",
    "def calculate_silhouette_score(clusters, n_dimensions):\n",
    "    \n",
    "    #This loop calculates the \"a-score\" of each sample\n",
    "    for cluster in clusters:    #For each cluster C\n",
    "        for sample in cluster.list_of_samples: #For each sample s in C\n",
    "            sample.silhouette_a = 0 #Attribute that contains the \"a-score\" of s\n",
    "            for sample_in_same_cluster in cluster.list_of_samples: #For each sample s' in C\n",
    "                if(not(sample.identifier == sample_in_same_cluster.identifier)): #If s != s'\n",
    "                    sample.silhouette_a += get_distance(sample, sample_in_same_cluster, n_dimensions) #Get the distance between s and s'\n",
    "            if(len(cluster.list_of_samples) > 1): #If |C| > 1, then calculates the \"a-score\" of s\n",
    "                sample.silhouette_a = float(sample.silhouette_a)/(len(cluster.list_of_samples)-1)\n",
    "    \n",
    "    #This loop calculates the \"b-score\" of each sample    \n",
    "    for cluster in clusters: #For each cluster C\n",
    "        for sample in cluster.list_of_samples: #For each sample s in C\n",
    "            min_b = math.inf #Variable that contains the minimum possible \"b-score\" of s\n",
    "            for other_cluster in clusters: #For each cluster C'\n",
    "                if(not(other_cluster.identifier == cluster.identifier)): #If C != C'\n",
    "                    current_b = 0 #Variable that contains \"b-score\" of s for C'\n",
    "                    for sample_in_other_cluster in other_cluster.list_of_samples: #For each sample s' in C'\n",
    "                        current_b += get_distance(sample, sample_in_other_cluster, n_dimensions) #Get the distance between s and s'\n",
    "                    current_b = float(current_b)/len(other_cluster.list_of_samples) #Calculates the \"b-score\" of s related to C'\n",
    "                    min_b = min(min_b, current_b) #Update the min_b, if it is the case\n",
    "            sample.silhouette_b = min_b #Attribute that contains the \"b-score\" of s\n",
    "            \n",
    "    silhouette_sample_scores = [] #Array that will contain the score of each sample\n",
    "    for cluster in clusters: #For each cluster C\n",
    "        for sample in cluster.list_of_samples: #For each sample s in C\n",
    "            sample.silhouette_score = (sample.silhouette_b - sample.silhouette_a)/max(sample.silhouette_a,sample.silhouette_b) #Calculates the score of s\n",
    "            silhouette_sample_scores.append(sample.silhouette_score) #Add the score of s to the array of scores\n",
    "\n",
    "    silhouette_score = statistics.mean(silhouette_sample_scores) #Calculate the final score of the clusterization, that is the average of all sample scores\n",
    "    \n",
    "    return silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that plots the average silhouette score for a fixed k, for each k in a given range\n",
    "\n",
    "def plot_silhouette_avg_score_graph(dataset, n_dimensions, min_k, max_k, n_repetitions):\n",
    "    \n",
    "    points = [] #Points of the graph\n",
    "    for k in range(min_k, max_k + 1): #For each value of k between min_k and max_k\n",
    "        average = 0 #Variable that will contain the silhouette average value for the fixed k\n",
    "        for i in range(n_repetitions): #Execute k-means n_repetitions times with same value of k\n",
    "            clusters, centroids, sse = kmeans(dataset, k, n_dimensions) #Run the k-means algorithm\n",
    "            silhouette_score = calculate_silhouette_score(clusters, n_dimensions) #Get the silhouette score of the clusterization\n",
    "            average += silhouette_score\n",
    "        average = float(average)/n_repetitions #Calculates the average score\n",
    "        points.append([k,average]) #Add the point (k, silhouette average value)\n",
    "    \n",
    "    #Plot the graph\n",
    "    x = []\n",
    "    y = []\n",
    "    for point in points:\n",
    "        x.append(point[0])\n",
    "        y.append(point[1])\n",
    "\n",
    "    plt.xlabel(\"K\", fontsize=15)\n",
    "    plt.ylabel(\"Silhouette average score\", fontsize=15)\n",
    "    plt.xticks(list(range(min_k,max_k + 1)))    \n",
    "    plt.grid(linestyle='--')\n",
    "    plt.plot(x,y,'o-', color='green')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that plots the silhouette graph of a given clusterization\n",
    "\n",
    "def plot_silhouette_graph(dataset, clusters, n_dimensions, k):\n",
    "    \n",
    "    #Calculate the silhouete score\n",
    "    silhouette_score = calculate_silhouette_score(clusters, n_dimensions)\n",
    "    \n",
    "    #The following code plots the silhouette graph and it was adapted from \n",
    "    #https://medium.com/neuronio/unsupervised-learning-with-k-means-3eaa0666eebf\n",
    "    \n",
    "    silhouette_sample_scores = [] #Array that will contain the score of each sample\n",
    "    for sample in dataset:\n",
    "        silhouette_sample_scores.append(sample.silhouette_score) #Add the score of s to the array of scores\n",
    "    min_silhouette_value = min(-0.1,min(silhouette_sample_scores))\n",
    "    fig, (ax1) = plt.subplots(1)\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    ax1.set_xlim([min_silhouette_value, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(dataset) + (k + 1) * 10])\n",
    "    # Compute the silhouette scores for each sample\n",
    "    y_lower = 10\n",
    "    for i,cluster in enumerate(clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = []\n",
    "        for sample in cluster.list_of_samples:\n",
    "            ith_cluster_silhouette_values.append(sample.silhouette_score)\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = len(ith_cluster_silhouette_values)\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        #color = plt.cm.nipy_spectral(float(i) / k)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,alpha=0.7)\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_score, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    x_ticks = []\n",
    "    j = 1\n",
    "    while j > min_silhouette_value:\n",
    "        x_ticks.append(j)\n",
    "        j = j - 0.1\n",
    "    x_ticks.append(min_silhouette_value)\n",
    "    ax1.set_xticks(x_ticks)\n",
    "    print(\"Silhouette score for k = \" + str(k) + \": \" + str(silhouette_score))\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data with \" + str(k) + \" clusters\"))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that plots a 2D graph of a clusterization\n",
    "\n",
    "def plot_2D_clusters(clusters, centroids):\n",
    "    for cluster in clusters:\n",
    "        x = []\n",
    "        y = []\n",
    "        for sample in cluster.list_of_samples:\n",
    "            x.append(sample.features[0])\n",
    "            y.append(sample.features[1])\n",
    "        plt.scatter(x, y)    \n",
    "    x = []\n",
    "    y = []\n",
    "    for centroid in centroids: #If the centroids of the k-means were given in the input, plot them too\n",
    "        x.append(centroid.features[0])\n",
    "        y.append(centroid.features[1])\n",
    "    plt.scatter(x, y, marker=\"x\", color=\"black\")\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### K-means algorithm ######\n",
    "\n",
    "#Function that assings each sample to a centroid\n",
    "\n",
    "def set_clusters(dataset, centroids, dimensions):\n",
    "    \n",
    "    for centroid in centroids: #Initialize the list of samples of each centroid as an empty list\n",
    "        centroid.list_of_samples = []\n",
    "        \n",
    "    for sample in dataset: #For each sample\n",
    "        closest_centroid = None #Variable that will contain the closest centroid\n",
    "        min_distance = math.inf\n",
    "        for centroid in centroids: #Check the distance of all centroids and select the closest\n",
    "            distance = get_distance(sample, centroid, dimensions)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_centroid = centroid\n",
    "        closest_centroid.list_of_samples.append(sample) #Add the sample to the list of samples of the closest centroid\n",
    "\n",
    "        \n",
    "#Function that calculates the Sum of the Squared Error\n",
    "\n",
    "def get_sse(centroids, dimensions):\n",
    "    sum = 0\n",
    "    for centroid in centroids:\n",
    "        for sample in centroid.list_of_samples:\n",
    "            sum = sum + get_distance(centroid, sample, dimensions)**2\n",
    "    return sum\n",
    "\n",
    "#Function that updates the features values of each centroid\n",
    "\n",
    "def update_centroids(centroids):\n",
    "    for centroid in centroids: #For each centroid\n",
    "        for i in range(len(centroid.features)): #For each feature of the centroid\n",
    "            centroid.features[i] = 0  \n",
    "            for j in range(len(centroid.list_of_samples)): #Collected the feature value of each sample assigned to the centroid\n",
    "                centroid.features[i] += centroid.list_of_samples[j].features[i]\n",
    "            if(len(centroid.list_of_samples) > 0): #If the cluster is not empty\n",
    "                centroid.features[i] = centroid.features[i]/float(len(centroid.list_of_samples)) #Apply the average value\n",
    "\n",
    "#Function that implements the k-means clusterization method\n",
    "\n",
    "def kmeans(dataset, n_clusters, dimensions):\n",
    "    \n",
    "    drawed_samples = random.sample(dataset, n_clusters) #Draw n_clusters samples to be the initial centroids\n",
    "    centroids = [] #Array of centroids\n",
    "    for i in range(n_clusters): #Add the initial centroids\n",
    "        new_centroid = Centroid(i+1, drawed_samples[i].features.copy())\n",
    "        centroids.append(new_centroid)\n",
    "    \n",
    "    set_clusters(dataset, centroids, dimensions) #Set the initial clusters\n",
    "    current_sse = get_sse(centroids, dimensions) #Calculate the inital error value\n",
    "    sse_converged = False #Flag that indicates if the error value converged\n",
    "    \n",
    "    iteration = 1\n",
    "    #print(\"Iteration #\" + str(iteration))\n",
    "    #plot_2D_clusters(centroids)\n",
    "    \n",
    "    while(not sse_converged): #Main loop of kmeans\n",
    "        update_centroids(centroids) #Update the centroid features values\n",
    "        set_clusters(dataset, centroids, dimensions) #Reset the clusters\n",
    "        new_sse = get_sse(centroids, dimensions) #Get the new error value\n",
    "        if new_sse < current_sse: #Check if the error value converged\n",
    "            current_sse = new_sse\n",
    "        else:\n",
    "            sse_converged = True\n",
    "        iteration += 1\n",
    "        #print(\"Iteration #\" + str(iteration))\n",
    "        #plot_2D_clusters(centroids)\n",
    "        \n",
    "    clusters = []\n",
    "    i = 1\n",
    "    for centroid in centroids:\n",
    "        new_cluster = Cluster(i, centroid.list_of_samples.copy())\n",
    "        clusters.append(new_cluster)\n",
    "        for sample in new_cluster.list_of_samples:\n",
    "            sample.cluster = new_cluster\n",
    "        i += 1\n",
    "    \n",
    "    return clusters, centroids, current_sse\n",
    "\n",
    "#Function that extends a clusterization made by k-means to cluster the points from the test set\n",
    "def extend_kmeans_clusterization(centroids, test_set, n_dimensions):\n",
    "    set_clusters(test_set, centroids, n_dimensions)\n",
    "    clusters = []\n",
    "    i = 1\n",
    "    for centroid in centroids:\n",
    "        new_cluster = Cluster(i, centroid.list_of_samples.copy())\n",
    "        clusters.append(new_cluster)\n",
    "        for sample in new_cluster.list_of_samples:\n",
    "            sample.cluster = new_cluster\n",
    "        i += 1\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-acrylic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Experiment Part 1\n",
    "\n",
    "\n",
    "#Testing K-means with 2-dimensional dataset\n",
    "\n",
    "dataset = get_dataset(\"cluster.dat\") #Build the dataset\n",
    "n_dimensions = 2 #Set the number of dimensions\n",
    "random.shuffle(dataset) #Randomize the dataset array\n",
    "#min_max_normalization(dataset, n_dimensions) #Apply the min-max normalization\n",
    "z_score_normalization(dataset, n_dimensions) #Apply the z-score normalization\n",
    "training_set, test_set = divide_dataset(dataset, 0.9, 0.1) #Partition the dataset into training set and test set\n",
    "\n",
    "plot_elbow_graph(training_set, n_dimensions,1, 10, 5) #Plot elbow graph\n",
    "plot_silhouette_avg_score_graph(training_set,n_dimensions, 2, 10, 5) #Plot silhouette average score graph\n",
    "\n",
    "for k in [3]:\n",
    "    clusters, centroids, sse = kmeans(training_set, k, n_dimensions)\n",
    "    plot_2D_clusters(clusters, centroids)\n",
    "    plot_silhouette_graph(training_set, clusters, n_dimensions, k)\n",
    "    clusters_test_set = extend_kmeans_clusterization(centroids, test_set, n_dimensions)\n",
    "    plot_2D_clusters(clusters_test_set, [])\n",
    "    plot_silhouette_graph(test_set, clusters_test_set, n_dimensions, k)\n",
    "\n",
    "\"\"\"\n",
    "#Testing K-means with 10-dimensional dataset\n",
    "dataset = get_dataset(\"trip_advisor.dat\") #Build the dataset\n",
    "n_dimensions = 10\n",
    "random.shuffle(dataset) #Randomize the dataset array\n",
    "#min_max_normalization(dataset, n_dimensions) #Apply the min-max normalization\n",
    "z_score_normalization(dataset, n_dimensions) #Apply the z-score normalization\n",
    "training_set, test_set = divide_dataset(dataset, 0.9, 0.1) #Partition the dataset into training set and test set\n",
    "plot_elbow_graph(training_set, n_dimensions,1, 10, 5) #Plot elbow graph\n",
    "plot_silhouette_avg_score_graph(training_set,n_dimensions, 2, 10, 5) #Plot silhouette average score graph\n",
    "\n",
    "for k in [2]:\n",
    "    clusters, centroids, sse = kmeans(training_set, k, n_dimensions)\n",
    "    plot_silhouette_graph(training_set, clusters, n_dimensions, k)\n",
    "    clusters_test_set = extend_kmeans_clusterization(centroids, test_set, n_dimensions)\n",
    "    plot_silhouette_graph(test_set, clusters_test_set, n_dimensions, k)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-shaft",
   "metadata": {},
   "source": [
    "# DBSCAN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class that represents a node for the DBSCAN method\n",
    "class DBSCAN_node:\n",
    "    def __init__(self, sample, label, cluster):\n",
    "        self.sample = sample \n",
    "        self.label = label   # (undefined | noise | core | border)\n",
    "        self.cluster = cluster # name of the cluster the node belongs to\n",
    "        self.neighborhood = set() \n",
    "        \n",
    "def DBSCAN_initialization(dataset):\n",
    "    DBSCAN_dataset = []\n",
    "    for sample in dataset:\n",
    "        new_node = DBSCAN_node(sample, \"undefined\", -1)\n",
    "        DBSCAN_dataset.append(new_node)\n",
    "    \n",
    "    return DBSCAN_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBSCAN_plot_2D_clusters(clusters):\n",
    "    for c in clusters:\n",
    "        x = []\n",
    "        y = []\n",
    "        for point in clusters[c]:\n",
    "            x.append(point.sample.features[0])\n",
    "            y.append(point.sample.features[1])\n",
    "        plt.scatter(x, y)    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def DBSCAN_find_neighborhood(dataset, dist_func, dim, eps, point):\n",
    "    neighbors = []\n",
    "    for n in dataset:\n",
    "        if dist_func(n.sample, point.sample, dim) <= eps:\n",
    "            neighbors.append(n)\n",
    "    \n",
    "    return set(neighbors)\n",
    "\n",
    "\n",
    "def DBSCAN_find_cluster(dataset, dist_func, dim, eps, minPoints, core, n_cluster):\n",
    "    cluster = []\n",
    "    to_explore = deque()\n",
    "    \n",
    "    cluster.append(core)\n",
    "    to_explore.append(core)\n",
    "    \n",
    "    # explore neighborhoods to expand the current cluster\n",
    "    while len(to_explore) > 0:\n",
    "        q = to_explore.popleft()\n",
    "        q.neighborhood = DBSCAN_find_neighborhood(dataset, dist_func, dim, eps, q)          \n",
    "        \n",
    "        # classify current point according to the size of its neighborhood\n",
    "        if len(q.neighborhood) >= minPoints: # current point is core\n",
    "            q.label = \"core\" \n",
    "            \n",
    "            # mark each neighbor as part of the current cluster \n",
    "            for r in q.neighborhood:\n",
    "                if r.label == \"undefined\" or r.label == \"noise\":\n",
    "                    r.cluster = n_cluster\n",
    "                    cluster.append(r)\n",
    "                    \n",
    "                    # classify current neighbor according to the size of its neighborhood\n",
    "                    if len(DBSCAN_find_neighborhood(dataset, dist_func, dim, eps, r)) >= minPoints:\n",
    "                        r.label = \"core\"\n",
    "                    else:\n",
    "                        r.label = \"border\"\n",
    "                    \n",
    "                    # add neighbor to the queue (if it is not there yet), so its neioghborhood will be analyzed\n",
    "                    if r not in to_explore:\n",
    "                        to_explore.append(r)\n",
    "                \n",
    "        else: # current point is border  \n",
    "            q.label = \"border\" \n",
    "    \n",
    "    return cluster\n",
    "    \n",
    "\n",
    "# DBSCAN implementation\n",
    "def DBSCAN(dataset, dist_func, dim, eps, minDensity):\n",
    "    # obtain a dataset in the convenient format\n",
    "    dataset = DBSCAN_initialization(dataset)\n",
    "    \n",
    "    # initialize auxiliary variables\n",
    "    n_clusters = 0   # name of the clusters\n",
    "    clusters = {}    # clusters obtained as the model\n",
    "    clusters[0] = [] # list to gather all outliers as one cluster\n",
    "    \n",
    "    for point in dataset:\n",
    "        if point.label == \"undefined\":\n",
    "            point.neighborhood = DBSCAN_find_neighborhood(dataset, dist_func, dim, eps, point)\n",
    "            \n",
    "            # check neighborhood of the current point\n",
    "            if len(point.neighborhood) >= minDensity: # neighborhood is large enough, current point is a core point\n",
    "                point.label = \"core\"\n",
    "                \n",
    "                # start a new cluster\n",
    "                n_clusters += 1 \n",
    "                point.cluster = n_clusters\n",
    "                clusters[n_clusters] = DBSCAN_find_cluster(dataset, dist_func, dim, eps, minDensity, point, n_clusters)\n",
    "            \n",
    "            else: # neighborhood not large enough, then classify current point as noise so that it may later be joined in some cluster\n",
    "                point.label = \"noise\"\n",
    "                point.cluster = 0\n",
    "                    \n",
    "    # create one cluster for all the outliers\n",
    "    for point in dataset:\n",
    "        if point.label == \"noise\":\n",
    "            clusters[0].append(point)\n",
    "\n",
    "    return dataset, clusters              \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-family",
   "metadata": {},
   "source": [
    "## Running DBSCAN for a two-dimensional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "ds_filepath = \"cluster.dat\"\n",
    "ds = get_dataset(ds_filepath)\n",
    "\n",
    "# Pre-process and divide the dataset into training and test sets\n",
    "random.seed(37154) \n",
    "random.shuffle(ds) \n",
    "training_set, test_set = divide_dataset(ds, 0.9, 0.1)\n",
    "z_score_normalization(training_set, 2)\n",
    "z_score_normalization(test_set, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Training step               #\n",
    "###############################\n",
    "\n",
    "# Run DBSCAN over the dataset using different values of minimum density and distance\n",
    "for density in range(0,16,5):\n",
    "    for eps in range(density, 0, -1):\n",
    "        model = []\n",
    "        clusters = {}\n",
    "        model, clusters = DBSCAN(training_set, get_distance, 2, eps/density, density)    \n",
    "        print(\"---------------------------------------------------------------------------------\")\n",
    "        print(\"\\033[1mConfiguration:\\033[0m minDensity = \", density, \", eps = \", eps/density)\n",
    "        print(\"There are\", len(clusters)-1, \"clusters and\", len(clusters[0]), \"outliers.\")\n",
    "        DBSCAN_plot_2D_clusters(clusters)\n",
    "\n",
    "print(\"\\033[1mConclusions:\\033[0m the value of the minimum density did not seem to affect the results in a high significant way, for this dataset. For minimum distance, however, if eps is too large, we could have some clusters containing several distant sets with high density, that is, some clusters that could have been separated in more different clusters. On the other hand, if eps is too small, we can end up with too many outliers; in the example, when eps<=0.3, all points were classified as outliers, which is not befitting with reality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Training step               #\n",
    "###############################\n",
    "\n",
    "print(\"\\n\\033[1mTunning the hiper-parameters:\\033[0m\")\n",
    "print(\"As a rule of thumb, we have chosen the minimum density to be 2 times the dimension of the dataset. Although, by observation of the results plotted above, we noticed we could have chosen a larger value, such as 15, for instance.\")\n",
    "print(\"As for the minimum distance, by the sole observation of the results plotted, we would have chosen the value 0.5.\")\n",
    "print(\"However, as a more automated mechanism, we used the k-nearest neighbor graph to find a proper value of minimum distance. In the following, we present an elbow graph of the fartherest distance in the range of the k-nearest neighbors of each point.\")\n",
    "\n",
    "def get_nearest_distances(dataset, dist_func, dim, k, point):\n",
    "    dist = []\n",
    "    for q in dataset:\n",
    "        dist.append(get_distance(q, point, dim))\n",
    "    \n",
    "    return sorted(dist)[:k]\n",
    "\n",
    "# # Tunning the hiper-parameters\n",
    "dim = len(training_set[0].features)\n",
    "k = 2*dim # set k to be 2 times the number of features minus one\n",
    "\n",
    "k_neighbors = {p : [] for p in training_set}\n",
    "for p in training_set:\n",
    "    k_neighbors[p] = get_nearest_distances(training_set, get_distance, dim, k, p) \n",
    "\n",
    "estimated_eps = [k_neighbors[p][k-1:] for p in training_set]\n",
    "estimated_eps = sorted(estimated_eps, reverse=True)\n",
    "\n",
    "# plot the elbow graph\n",
    "x = [i for i in range(len(estimated_eps))]\n",
    "y = estimated_eps\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"distance\")\n",
    "plt.plot(x, y, 'g-')    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Validation step             #\n",
    "###############################\n",
    "\n",
    "# set hiper-parameters with the appropiate values\n",
    "best_eps = 0.16\n",
    "best_density = 4 #2*dim\n",
    "\n",
    "# Validate the model obtained using proper values of the hiper-parameters\n",
    "print(\"Based on the analysis of the elbow graph showed above, we chose the hiper-parameters to be:\")\n",
    "print(\"minimum density = \", best_density, \"and minimun distance = \", best_eps, \"\\n\")\n",
    "\n",
    "# classify the test set according to the model obtained with the traininset\n",
    "print(\"Now we will classify the test set. We use two approaches. In the fisrt one, we simply run the DBSCAN method over the test set using the configuration we found to be the most appropiate in the training step.\")\n",
    "print(\"In the second approache, for each point of the test set, we find what its neighborhood would be in the training set, and then we count the number of times that each cluster occurs in this neighborhood. Finally, we assign the most frequent cluster in this neighborhood as the cluster of the point of the test set.\")\n",
    "print(\"Observing the graphs ploted bellow, we can conclude that the second approach gave much better results than the first one.\")\n",
    "\n",
    "# first approach\n",
    "print(\"\\n\\033[1mFisrt approach:\\033[0m run DBSCAN for the test set with the most convenient configuration obtained from the training step.\")\n",
    "model = []\n",
    "clusters_1 = {}\n",
    "testset_labeled, clusters_1 = DBSCAN(test_set, get_distance, 2, best_eps, best_density / len(test_set))\n",
    "DBSCAN_plot_2D_clusters(clusters_1)\n",
    "\n",
    "# second approach\n",
    "print(\"\\n\\033[1mSecond approach:\\033[0m assign to each point in the test set, the \\\"most frequent cluster\\\" in the neighborhood of the point when considered the training set.\")\n",
    "# initialization of data set and auxiliary variables\n",
    "v_set = DBSCAN_initialization(test_set)\n",
    "neighbors_count = {key: 0 for key in range(0,50)}\n",
    "v_clusters = {key: [] for key in range(0,50)}\n",
    "\n",
    "# get model generated by training the training set\n",
    "model, clusters_2 = DBSCAN(training_set, get_distance, 2, best_eps, best_density)  \n",
    "\n",
    "# classify each point of the test set one of the clusters of the model\n",
    "for v_point in v_set:\n",
    "    # get neighborhood of the current point as if it was placed in the model set\n",
    "    v_neighborhood = DBSCAN_find_neighborhood(model, get_distance, 2, best_eps, v_point)\n",
    "    \n",
    "    # count the number of neighbors of each cluster\n",
    "    for q in v_neighborhood:\n",
    "        neighbors_count[q.cluster] += 1\n",
    "    \n",
    "    # set current point to the \"most frequent\" cluster in its \"model neighborhood\"\n",
    "    v_point.cluster = max(neighbors_count, key=neighbors_count.get)\n",
    "    \n",
    "    # reset the counter of neighbors of each cluster\n",
    "    neighbors_count = {key: 0 for key in neighbors_count}\n",
    "    \n",
    "# construct the dictionary with all clusters and plot the result\n",
    "for p in v_set:\n",
    "    v_clusters[p.cluster].append(p)\n",
    "DBSCAN_plot_2D_clusters(v_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-retro",
   "metadata": {},
   "source": [
    "## Running DBSCAN for a n-dimensional dataset (n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "ds_filepath = \"trip_advisor.dat\"\n",
    "#ds_filepath = \"wimbledon_men_2013.dat\"\n",
    "ds = get_dataset(ds_filepath)\n",
    "dim = len(training_set[0].features) # get the number of features of the dataset\n",
    "\n",
    "# Pre-process the dataset\n",
    "random.seed(37154) \n",
    "random.shuffle(ds)\n",
    "training_set, test_set = divide_dataset(ds, 0.9, 0.1) \n",
    "z_score_normalization(training_set, dim)\n",
    "z_score_normalization(test_set, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Training step               #\n",
    "###############################\n",
    "\n",
    "# Tuning hiper-parameters\n",
    "dim = len(training_set[0].features)\n",
    "k = 2*dim # set k to be 2 times the number of features minus one\n",
    "\n",
    "k_neighbors = {p : [] for p in training_set}\n",
    "for p in training_set:\n",
    "    k_neighbors[p] = get_nearest_distances(training_set, get_distance, dim, k, p) \n",
    "\n",
    "estimated_eps = [k_neighbors[p][k-1:] for p in training_set]\n",
    "estimated_eps = sorted(estimated_eps, reverse=True)\n",
    "\n",
    "# plot the elbow graph\n",
    "x = [i for i in range(len(estimated_eps))]\n",
    "y = estimated_eps\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"distance\")\n",
    "plt.plot(x, y, 'g-')    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Validation step             #\n",
    "###############################\n",
    "\n",
    "# set hiper-parameters with the appropiate values\n",
    "best_eps = 3.5\n",
    "best_density = 2*dim\n",
    "\n",
    "# first approach\n",
    "print(\"\\nFisrt approach: run DBSCAN for the test set with the most convenient configuration obtained from the training step.\")\n",
    "\n",
    "clusters_1 = {}\n",
    "testset_labeled, clusters_1 = DBSCAN(test_set, get_distance, dim, best_eps, best_density / len(test_set))\n",
    "# DBSCAN_plot_2D_clusters(clusters)\n",
    "print(\"test set, approach 1, number of clusters: \", len(clusters_1))\n",
    "\n",
    "# second approach\n",
    "print(\"\\nSecond approach: assign to each point in the test set, the \\\"most frequent cluster\\\" in the neighborhood of the point when considered the training set.\")\n",
    "# obtain the model by rinning DBSCAN over the training set with the best configuration of parameters\n",
    "model = []\n",
    "clusters = {}\n",
    "model, clusters = DBSCAN(training_set, get_distance, dim, best_eps, best_density)\n",
    "print(\"training set, number of clusters: \", len(clusters))\n",
    "\n",
    "# initialization of data set and auxiliary variables\n",
    "v_set = DBSCAN_initialization(test_set)\n",
    "neighbors_count = {key: 0 for key in range(0,len(test_set))}\n",
    "v_clusters = {key: [] for key in range(0,len(test_set))}\n",
    "\n",
    "# run through each point of the test set so as to classify them\n",
    "for v_point in v_set:\n",
    "    # get neighborhood of the current point as if it was placed in the model set\n",
    "    v_neighborhood = DBSCAN_find_neighborhood(model, get_distance, dim, best_eps, v_point)\n",
    "    \n",
    "    # count the number of neighbors of each cluster\n",
    "    for q in v_neighborhood:\n",
    "        neighbors_count[q.cluster] += 1\n",
    "    \n",
    "    # set current point to the \"most frequent\" cluster in its \"model neighborhood\"\n",
    "    v_point.cluster = max(neighbors_count, key=neighbors_count.get)\n",
    "    \n",
    "    # reset the counter of neighbors of each cluster\n",
    "    neighbors_count = {key: 0 for key in neighbors_count}\n",
    "    \n",
    "# construct the dictionary with all clusters and plot the result\n",
    "for p in v_set:\n",
    "    v_clusters[p.cluster].append(p)\n",
    "    \n",
    "print(\"teset set, number of clusters: \", len(v_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

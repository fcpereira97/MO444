{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vanilla-paintball",
   "metadata": {},
   "source": [
    "# <center>MO444 - Aprendizado de Máquina e Reconhecimento de Padrões</center>\n",
    "## <center>Primeiro semestre de 2021</center>\n",
    "## <center>Projeto 3: Algoritmos Evolucionários e Aprendizado por Reforço</center>\n",
    "## <center>Discentes: Elisa Dell'Arriva (135551) e Felipe de Carvalho Pereira (230214)</center>\n",
    "_________________________________________________________________________\n",
    "\n",
    "# 1 Introdução\n",
    "\n",
    "O presente *notebook* consiste no terceiro projeto avaliativo da disciplina MO444 - Aprendizado de Máquina e Reconhecimento de Padrões, ministrada pela Profa. Dra. Esther Colombini. O principal objetivo da atividade foi o de implementar e experimentar técnicas de Algoritmos Evolucionários e de Aprendizado por Reforço para resolver o Problema do Pacman.<br>\n",
    "\n",
    "Na primeira parte do projeto, descrita na Seção 2, buscamos resolver o Problema do Pacman por meio de um algoritmo de Programação Genética baseado nos métodos empregados em [4] e [5]. Na segunda etapa, descrita na Seção 3, lidamos com o mesmo problema, mas empregamos uma técnica de Aprendizado por Reforço denoniminada Q-learning, de maneira semelhante ao que é apresentado em [6]. Na Seção 4, realizamos um experimento adicional a fim de comparar os algoritmos desenvolvidos nas Seções anteriores.\n",
    "\n",
    "## 1.1 O Problema do Pacman\n",
    "\n",
    "O famoso jogo do Pacman se passa em um labirinto, na forma de um tabuleiro, em que o Pacman, o agente controlado pelo usuário, movimenta-se por meio das células (ou casas). O objetivo do jogo é coletar todos os itens de comida dispostos no labirinto. Além do Pacman, existem outros agentes que se movimentam pelo labirinto. Estes são denominados fantasmas e não são controlados pelo usuário. O jogo acaba somente em duas ocasiões: quando todos os itens de comida forem consumidos e, neste caso, decreta-se vitória; ou quando o Pacman colide com algum fantasma, e neste caso, decreta-se derrota.\n",
    "\n",
    "Além dos itens de comida, existem cápsulas que também são consumíveis pelo Pacman. No momento em que uma cápsula é consumida, a velocidade com a qual os fantasmas se movimentam é reduzida, e os mesmos tornam-se comestíveis pelo Pacman. Contudo, tais efeitos persistem por um curto intervalo de tempo. Quando um fantasma está sob o efeito da cápsula, dizemos que ele está *comestível*, caso contrário dizemos que ele está *ativo*. A pontuação do usuário durante o jogo é baseada nas seguintes regras:\n",
    "\n",
    "* A pontuação no início do jogo é igual a 0;\n",
    "* A cada iteração do jogo, a pontuação é decrementada em uma unidade;\n",
    "* Quando um item de comida é consumido, a pontuação é incrementada em 10 unidades;\n",
    "* Quando um fantasma comestível é consumido, a pontuação é incrementada em 200 unidades;\n",
    "* Quando o Pacman colide com um fantasma ativo a pontuação é decrementada em 500 unidades;\n",
    "* Quando o Pacman consome o último item de comida, a pontuação é incrementada em 500 unidades.\n",
    "\n",
    "Nesse contexto, o Problema do Pacman corresponde em desenvolver um agente capaz de controlar o Pacman com o objetivo de vencer o jogo e maximizar a pontuação obtida.\n",
    "\n",
    "Junto à especificação deste projeto, foi fornecida uma API que implementa o jogo do Pacman na linguagem Python 2. Contudo, uma versão em Python 3 desta mesma API está disponível em [7] e esta última foi aquela empregada neste trabalho. Em particular, os mapas (ou *layouts*) utilizados foram: smallClassic, mediumClassic e originalClassic.\n",
    "\n",
    "# 2 Parte $I$ - Modelo Evolucionário\n",
    "\n",
    "Nesta Seção, descrevemos a abordagem baseada em modelos evolucionários utilizada neste trabalho para resolver o Problema do Pacman.\n",
    "\n",
    "## 2.1 Algoritmos Genéticos\n",
    "\n",
    "Algoritmos genéticos são abordagens heurísticas nas quais modelam-se soluções de um determinado problema como indivíduos de uma população. Em cada iteração do algoritmo, tem-se uma geração (no sentido temporal) da população. Ao longo das gerações, os indivíduos se reproduzem e sofrem mutações, de modo análogo ao que é observado na natureza com diversas espécies. Em particular, o algoritmo aplica um procedimento semelhante ao processo de Seleção Natural, de tal modo que apenas os indivíduos mais adaptados possam sobreviver através das gerações.\n",
    "\n",
    "A maneira como soluções são representadas por indivíduos varia de acordo com o problema tratado. Além disso, a literatura (vide [1,2,3]) apresenta diversas maneiras de se implementar as etapas de reprodução (*crossover*), mutação e substituição de população. Em linhas gerais, um algoritmo genético funciona da seguinte maneira.\n",
    "\n",
    "* 1 - Uma população inicial de indivíduos (possivelmente produzidos de maneira aleatorizada) é estabelecida;\n",
    "* 2 - Pares de indivíduos (ou pais) são selecionados para reprodução e novos indivíduos (filhos) são produzidos, os quais herdam características de ambos os pais;\n",
    "* 3 - Uma fração da população é escolhida para sofrer mutações, isto é, alterações randomizadas em sua estrutura;\n",
    "* 4 - Cada indivíduo é avaliado de acordo com uma fução de aptidão (ou *fitness*);\n",
    "* 5 - Os indivíduos considerados mais aptos são escolhidos para compor a próxima geração da população;\n",
    "* 6 - Repete-se os passos 2 a 5 até que a população convirja ou que o limite de iterações preestabelecido seja atingido.\n",
    "\n",
    "## 2.2 Programação Genética\n",
    "\n",
    "Neste trabalho, escolhemos a técnica denominada Programação Genética para resolver o Problema do Pacman. A Progração Genética pode ser entendida como uma variação dos algoritmos genéticos em que cada indivíduo da população corresponde a um programa (*software*) ao invés de uma solução propriamente dita. Quando executado, o programa (indivíduo) deve ser capaz de produzir uma solução para o problema.\n",
    "\n",
    "No contexto do jogo do Pacman, um indivíduo consiste em uma árvore de decisão por meio da qual chega-se a uma escolha de movimentação do Pacman, considerando o estado corrente do jogo. Denominaremos esse tipo de árvore de decisão como *política*, i.e., uma política de movimentação do Pacman. Para cada iteração do jogo tem-se as informações a respeito da disposição dos itens e agentes no labirinto. Assim, quando uma política é executada, a mesma se utiliza das informações do labirinto para decidir o próximo movimento do Pacman (norte, sul, leste, oeste ou ficar parado). \n",
    "\n",
    "A seguir, descrevemos a modelagem de Programação Genética que empregamos para resolver o Problema do Pacman, a qual foi baseada nos algoritmos desenvolvidos em [4,5].\n",
    "\n",
    "### 2.2.1 Modelagem do indivíduo\n",
    "\n",
    "Cada indivíduo da população corresponde a uma política e é implementado por meio de árvores (estrutura de dados). Cada nó da árvore tem uma única natureza entre as que são descritas abaixo:\n",
    "\n",
    "* Data: valor numérico que pode ser uma constante ou uma informação acerca do atual estado do tabuleiros;\n",
    "* Action: representa o objetivo que baseará a próxima ação do Pacman;\n",
    "* Iflte: (*If less then else*): nó que possui quatro filhos, sendo os dois primeiros da natureza Data e os últimos da natureza Action ou Iflte.\n",
    "\n",
    "Se um nó é do tipo Data, então ele tem alguma das seguintes subnaturezas:\n",
    "\n",
    "* DistToFood: a menor das distâncias de Manhattan entre a posição do Pacman e as posições dos itens de comida restantes;\n",
    "* DistToCapsule: a menor das distâncias de Manhattan entre a posição do Pacman e as posições das cápsulas restantes;\n",
    "* DistToActiveGhost: a menor das distâncias de Manhattan entre a posição do Pacman e as posições dos fantasmas ativos;\n",
    "* DistToEdibleGhost: a menor das distâncias de Manhattan entre a posição do Pacman e as posições dos fantasmas comestíveis;\n",
    "* Constant: um valor constante entre 1 e o *maxConst*, onde *maxConst* é um parâmetro.\n",
    "\n",
    "Se um nó é do tipo Action, então ele tem alguma das seguintes subnaturezas:\n",
    "\n",
    "* GetFood: Pacman deve se movimentar em direção ao item de comida mais próximo;\n",
    "* GetCapsule: Pacman deve se movimentar em direção à capsula mais mais próxima;\n",
    "* EscapeFromGhost: Pacman deve se movimentar na direção que o deixa mais distintante do fantasma mais próximo;\n",
    "* GetEdibleGhost: Pacman deve se movimentar em direção à capsula mais mais próxima.\n",
    "\n",
    "Apenas nós do tipo Iflte possuem filhos e, em particular, a raiz da árvore de decisão é sempre do tipo Iflte.\n",
    "\n",
    "### 2.2.2 *Parsing* do indivíduo\n",
    "\n",
    "Por meio da API do Pacman, criamos uma classe de agente que implementa (sobrescreve) a função *getAction*. A função *getAction* recebe o objeto *state* e deve retornar a ação que o Pacman irá realizar, i.e., ir para a posição adjacente ao norte, sul, leste ou oeste ou ainda permanecer na posição atual. O objeto *state* é útil para coletar informações acerca do atual estado do labirinto. Observe que durante uma partida do jogo, a função *getAction* é invocada a cada iteração do jogo.\n",
    "\n",
    "Em nossa modelagem, a função *getAction* invoca uma função de *parsing* que recebe o objeto *state* e uma política. Na função de *parsing*, a árvore de decisão (política) é percorrida a partir da raiz até algum nó folha que é uma ação. A ação determina um movimento do Pacman que é retornado pelo *parser* e, em seguida retornado pela função *getAction*.\n",
    "\n",
    "A seguir, detalharemos a função de *parsing*. Primeiramente, visitamos o nó raiz da política. Como já mencionamos, trata-se de um nó do tipo Iflte. Para esse tipo, faz-se o que segue. Primeiro avaliamos o valor numérico dos dois primeiros filhos do nó raiz, os quais são do tipo Data. Se o valor do primeiro filho for menor que o do segundo, então o terceiro filho do nó raiz é o próximo a ser visitado, caso contrário, o quarto filho é o próximo a ser visitado. Como vimos, esses dois últimos só podem ser do tipo Action ou Iflte. Se o próximo nó for do tipo Iflte, repete-se o procedimento descrito anteriormente. Caso contrário, se for do tipo Action, então é executado um procedimento que determina a ação do Pacman e o *parsing* é encerrado.\n",
    "\n",
    "Quando o valor de um nó do tipo Data é avaliado, faz-se o que segue de acordo com a subnatureza do nó. Se a subnatureza é Constant, então já existe um valor numérico constante associado àquele nó, o qual é retornado. Caso contrário, trata-se de uma subnatureza que é a distância do Pacman para objetos do mesmo tipo: comida, cápsula, fantasma ativo ou comestível. Nesse caso, calcula-se a distância (de Manhattan) entre o Pacman e todos os objetos do daquele tipo e retorna-se a menor das distâncias. Por exemplo, se a subnatureza é DistToFood, então é retornada a distância (de Manhattan) entre a posição do Pacman e a posição do item de comida mais próximo (considerando a distância de Manhattan).\n",
    "\n",
    "Quando um nó do tipo Ação é visitado, faz-se o que segue de acordo com a subnatureza do nó. Se a subnatureza é EscapeFromGhost, então calculamos a distância (de Manhattan) entre a posição do Pacman e o fantasma ativo mais próximo (considerando a distância de Manhattan). Em seguida, escolhemos a movimentação do Pacman que aumenta ou mantém a distância em relação àquele fantasma. Na prática isso significa fugir daquele fantasma com uma escolha gulosa de movimentação. Se a subnatureza é diferente de EscapeFromGhost, trata-se de uma subnatureza em que o Pacman objetivará comer algum item: comida, cápsula ou fantasma comestível. Nesse caso, calcula-se o caminho mais curto no labirinto entre o Pacman e o item mais próximo. O movimento do Pacman será aquele que vai em direção ao item nesse caminho. Para calcular esses caminhos, implementamos uma estrutura de dados adicional em que as células acessíveis do labirinto são vértices de um grafo e, dessa forma, a menor caminho entre duas células é calculado por uma busca em largura naquele grafo. Observe que sempre que um menor caminho entre duas células é calculado, o armazenamos em memória para não que não seja necessário recalculá-lo, no caso em que o cenário venha a se repetir. No caso especial em que não restam itens do tipo pelo qual ação objetiva alcançar, a ação padrão tomada é GetFood.\n",
    "\n",
    "### 2.2.3 Operações genéticas\n",
    "\n",
    "Adiante, descrevemos as operações relativas a evolução da população, tais como *crossover*, mutação e substituição, que são adotadas no nosso modelo. A população inicial é produzida com indivíduos gerados de maneira aleatorizada com profundidade da árvore de decisão limitada a um parâmetro preestabelecido. Além disso, o tamanho $n$ da população também é parametrizado, porém é fixo para todas as iterações do algoritmo.\n",
    "\n",
    "Seja $P$ o conjunto de $n$ indivíduos da $i$-ésima geração. O primeiro passo é fazer com que os indivíduos se reproduzam. Para isso, selecionamos dois indivíduos $p_1$ e $p_2$ de $P$ de maneira puramente aleatória. Em seguida, criamos uma cópia $f_1$ de $p_1$ e uma cópia $f_2$ de $p_2$. Depois, elencamos uma lista de nós do tipo Iflte (excetuando o nó raiz) de $f_1$ e fazemos o mesmo para $f_2$. Mais à frente, sorteamos de tais listas (de maneira puramente aleatória) um nó Iflte de $f_1$ e um nó Iflte de $f_2$ e trocamos, entre $f_1$ e $f_2$, as subárvores enraizadas nesses nós. Por fim acrescentamos $f_1$ e $f_2$ a população $P$. Repetimos esse procedimento até que o total de filhos gerados atinja um valor parametrizado.\n",
    "\n",
    "Em seguida, aplicamos o processo de mutação. Para isso, selecionamos um subconjunto $X$ de $P$ de indivíduos de $P$ e para cada $x \\in X$, criamos uma cópia $x'$ de $x$ e alteramos $x$ da seguinte maneira. Elencamos uma lista de nós de $x'$ que não são do tipo Iflte e sorteamos um deles. Se o nó for do tipo Action, alteramos a subnatureza do nó para uma subnatureza diferente e de maneira aleatorizada. Se o nó for do tipo Data, temos dois casos. Se a subnatureza for Constant, então alteramos aleatoriamente o valor da constante associada ao nó. Caso contrário, alteramos a subnatureza do nó para alguma que é diferente da atual e diferente de Constant. Em seguida adicionamos $x$ a $P$.\n",
    "\n",
    "O penúltimo passo é avaliar o *fitness* de cada indivíduo de $P$. Antes de descrever a função de *fitness*, precisamos descrever o que consideramos como pontuação final de um jogo. Em termos de vitória ou derrota, mantemos a pontuação original do jogo: o Pacman vence se comer todos os itens de comida e perde se colidir com algum fantasma ativo. Contudo, alteramos o cálculo a pontuação propriamente dita. Em caso de derrota, a pontuação final é aquela retornada pela API. Em caso de vitória, somamos 10.000 pontos à pontuação retornada pela API. A razão dessa adoção é baseada no fato de que, se mativermos a pontuação original, em muitos casos é possível que o agente tenha sido derrotado e termine com uma pontuação maior do que se tivesse vencido. Assim, os 10.000 pontos adicionais servem para termos uma maior discrepância na pontuação entre os casos de vitória e derrota. Ademais, no nosso modelo, o *fitness* de um indivíduo é igual à média das pontuações de 10 execuções do jogo com sementes (de geração de número pseudoaleatório) distintas.\n",
    "\n",
    "Por fim, os $n$ indivíduos de $P$ com maior *fitness* são escolhidos para constituir a $i+1$-ésima geração.\n",
    "\n",
    "## 2.3 Código-Fonte\n",
    "\n",
    "A seguir, temos o código-fonte relativo à Parte I do projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "perfect-narrative",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-650a133095cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;31m# Variáveis globais\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_cells\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_of_cells\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy_moves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;31m# Imprime uma árvore de decisão.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Importação de módulos\n",
    "from pacman import *\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from game import Directions\n",
    "from game import Agent\n",
    "import copy\n",
    "import math\n",
    "\n",
    "###############################################################################\n",
    "# Implementações relativas à obtenção de caminhos entre células do tabuleiro. #\n",
    "###############################################################################\n",
    "\n",
    "# Classe que representa uma célula do labirinto\n",
    "class Cell:\n",
    "    coordinates = None\n",
    "    adjacency = np.array([])\n",
    "    visited = False\n",
    "    parent = None\n",
    "\n",
    "# Função que retorna lista de células do grafo que representa o labirinto\n",
    "# e dicionário de que mapeia as coordenadas das células nos objetos que são células.\n",
    "def get_list_and_dict_of_cells(layout):\n",
    "    dict_of_cells = dict([])\n",
    "    list_of_cells = np.array([])\n",
    "\n",
    "    for x in range(layout.width):\n",
    "        for y in range(layout.height):\n",
    "            if not layout.walls[x][y]:\n",
    "                new_cell = Cell()\n",
    "                new_cell.coordinates = (x,y)\n",
    "                dict_of_cells[str(x) + ',' + str(y)] = new_cell\n",
    "                list_of_cells = np.append(list_of_cells, new_cell)\n",
    "\n",
    "    for cell in list_of_cells:\n",
    "        x = cell.coordinates[0]\n",
    "        y = cell.coordinates[1]\n",
    "\n",
    "        if x - 1 >= 0 and not layout.walls[x-1][y]:\n",
    "            cell.adjacency = np.append(cell.adjacency, dict_of_cells[str(x-1) + ',' + str(y)])\n",
    "\n",
    "        if y - 1 >= 0 and not layout.walls[x][y-1]:\n",
    "            cell.adjacency = np.append(cell.adjacency, dict_of_cells[str(x) + ',' + str(y-1)])\n",
    "\n",
    "        if x + 1 <= layout.width and not layout.walls[x+1][y]:\n",
    "            cell.adjacency = np.append(cell.adjacency, dict_of_cells[str(x+1) + ',' + str(y)])\n",
    "\n",
    "        if y + 1 <= layout.height and not layout.walls[x][y+1]:\n",
    "            cell.adjacency = np.append(cell.adjacency, dict_of_cells[str(x) + ',' + str(y+1)])\n",
    "    \n",
    "    return list_of_cells, dict_of_cells\n",
    "    \n",
    "# Função que executa uma busca em profundidade no grafo que representa o labirinto\n",
    "# e determina o menor caminho no labirinto entre essas duas células.\n",
    "def find_shortest_path(initial_cell_coordinates, final_cell_coordinates, list_of_cells, dict_of_cells):\n",
    "    \n",
    "    initial_cell = dict_of_cells[str(initial_cell_coordinates[0]) + ',' + str(initial_cell_coordinates[1])]\n",
    "    final_cell = dict_of_cells[str(final_cell_coordinates[0]) + ',' + str(final_cell_coordinates[1])]\n",
    "    \n",
    "    for cell in list_of_cells:\n",
    "        cell.visited = False\n",
    "        cell.parent = None\n",
    "    \n",
    "    queue = []\n",
    "    queue.append(initial_cell)\n",
    "    \n",
    "    while not final_cell.visited:\n",
    "        current_cell = queue.pop(0)\n",
    "        current_cell.visited = True\n",
    "        for adjacent_cell in current_cell.adjacency:\n",
    "            if not adjacent_cell.visited:\n",
    "                queue.append(adjacent_cell)\n",
    "                adjacent_cell.parent = current_cell\n",
    "                \n",
    "    path = []\n",
    "    current_cell = final_cell\n",
    "    while current_cell != initial_cell:\n",
    "        path.append(current_cell)\n",
    "        current_cell = current_cell.parent\n",
    "    path.append(current_cell)\n",
    "    \n",
    "    path.reverse()\n",
    "    \n",
    "    for i in range(len(path)-1):\n",
    "        direction = None\n",
    "        if path[i].coordinates[0] > path[i+1].coordinates[0]:\n",
    "            direction = Directions.WEST\n",
    "        elif path[i].coordinates[0] < path[i+1].coordinates[0]:\n",
    "            direction = Directions.EAST\n",
    "        elif path[i].coordinates[1] > path[i+1].coordinates[1]:\n",
    "            direction = Directions.SOUTH\n",
    "        else:\n",
    "            direction = Directions.NORTH\n",
    "        key = str(path[i].coordinates[0]) + ',' + str(path[i].coordinates[1]) + '-' + str(final_cell.coordinates[0]) + ',' + str(final_cell.coordinates[1])\n",
    "        greedy_moves[key] = direction\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# Implementações relativas ao agente do Pacman (Programação Genética) #\n",
    "#######################################################################\n",
    "\n",
    "# Listas de naturezas e subnaturezas\n",
    "node_natures = ['Iflte', 'Action', 'Data']\n",
    "data_subnatures = ['DistToFood', 'DistToCapsule', 'DistToActiveGhost', 'DistToEdibleGhost', 'Constant']\n",
    "action_subnatures = ['GetFood', 'GetCapsule', 'EscapeFromGhost', 'GetEdibleGhost']\n",
    "\n",
    "# Classe que representa um nó de uma árvore de decisão,\n",
    "# i.e. o nó de uma política, ou ainda, o nó de um indivíduo.\n",
    "class Node:\n",
    "    isRoot = False\n",
    "    nature = None\n",
    "    subnature = None\n",
    "    value = None\n",
    "    parent = None\n",
    "    children = np.array([])\n",
    "    fitness = None\n",
    "    win_counter = None\n",
    "    \n",
    "# Classe que implementa o agente.\n",
    "# O único atributo, policy, deve receber o a árvore de decisão.\n",
    "class GpAgent(Agent):\n",
    "\n",
    "    # Recebe a árvore de decisão (política).\n",
    "    policy = None\n",
    "    \n",
    "    # Função que encontra o item de comida mais próximo do Pacman.\n",
    "    def find_closest_food(self, state):\n",
    "        food_list = state.getFood().asList()\n",
    "        closest_food_coordinates = None\n",
    "        closest_food_dist = math.inf\n",
    "        for i in range(len(food_list)):\n",
    "            current_dist = manhattanDistance(state.getPacmanPosition(), food_list[i])\n",
    "            if current_dist < closest_food_dist:\n",
    "                closest_food_dist = current_dist\n",
    "                closest_food_coordinates = food_list[i]\n",
    "        return closest_food_coordinates, closest_food_dist\n",
    "    \n",
    "    # Função que encontra a cápsula mais próxima do Pacman.\n",
    "    def find_closest_capsule(self, state):\n",
    "        capsule_list = state.getCapsules()\n",
    "        closest_capsule_coordinates = None\n",
    "        closest_capsule_dist = math.inf\n",
    "        for i in range(len(capsule_list)):\n",
    "            current_dist = manhattanDistance(state.getPacmanPosition(), capsule_list[i])\n",
    "            if current_dist < closest_capsule_dist:\n",
    "                closest_capsule_dist = current_dist\n",
    "                closest_capsule_coordinates = capsule_list[i]\n",
    "        return closest_capsule_coordinates, closest_capsule_dist\n",
    "    \n",
    "    # Função que encontra o fantasma ativo mais próximo do Pacman.\n",
    "    def find_closest_active_ghost(self, state):\n",
    "        closest_ghost_coordinates = None\n",
    "        closest_ghost_dist = math.inf\n",
    "        for i in range(1, state.getNumAgents()):\n",
    "            if state.data.agentStates[i].scaredTimer <= 1:\n",
    "                ghost_position = state.getGhostPosition(i)\n",
    "                ghost_position = (int(ghost_position[0]), int(ghost_position[1]))\n",
    "                current_dist = manhattanDistance(state.getPacmanPosition(), ghost_position)\n",
    "                if current_dist < closest_ghost_dist:\n",
    "                    closest_ghost_dist = current_dist\n",
    "                    closest_ghost_coordinates = ghost_position\n",
    "        return closest_ghost_coordinates, closest_ghost_dist\n",
    "    \n",
    "    # Função que encontra o fantasma comestível mais próximo do Pacman.\n",
    "    def find_closest_edible_ghost(self, state):\n",
    "        closest_ghost_coordinates = None\n",
    "        closest_ghost_dist = math.inf\n",
    "        for i in range(1, state.getNumAgents()):\n",
    "            if state.data.agentStates[i].scaredTimer > 1:\n",
    "                ghost_position = state.getGhostPosition(i)\n",
    "                ghost_position = (int(ghost_position[0]), int(ghost_position[1]))\n",
    "                current_dist = manhattanDistance(state.getPacmanPosition(), ghost_position)\n",
    "                if current_dist < closest_ghost_dist:\n",
    "                    closest_ghost_dist = current_dist\n",
    "                    closest_ghost_coordinates = ghost_position\n",
    "        return closest_ghost_coordinates, closest_ghost_dist    \n",
    "    \n",
    "    \n",
    "    # Função que faz o parsing da política carregada no atributo policy.\n",
    "    def parse_policy(self, policy, state): \n",
    "        \n",
    "        current_node = policy # Carrega a raiz da árvore de decisão \n",
    "        \n",
    "        # Percorre a árvore de acordo com a descrição feita na Seção 2.2.2\n",
    "        if current_node.nature == 'Iflte':\n",
    "            children = current_node.children\n",
    "            for child in children[:2]:\n",
    "                if child.subnature == 'DistToFood':\n",
    "                    child.value = self.find_closest_food(state)[1]\n",
    "                elif child.subnature == 'DistToCapsule':\n",
    "                    child.value = self.find_closest_capsule(state)[1]\n",
    "                elif child.subnature == 'DistToActiveGhost':\n",
    "                    child.value = self.find_closest_active_ghost(state)[1]\n",
    "                elif child.subnature == 'DistToEdibleGhost':\n",
    "                    child.value = self.find_closest_edible_ghost(state)[1]\n",
    "            if children[0].value < children[1].value:\n",
    "                return self.parse_policy(children[2], state)\n",
    "            else:\n",
    "                return self.parse_policy(children[3], state)\n",
    "        elif current_node.nature == 'Action':\n",
    "            subnature = current_node.subnature \n",
    "            if subnature == 'GetCapsule':\n",
    "                closest_capsule_coordinates = self.find_closest_capsule(state)[0]\n",
    "                if closest_capsule_coordinates != None:\n",
    "                    current_position = list(state.getPacmanPosition())\n",
    "                    key = str(current_position[0]) + ',' + str(current_position[1]) + '-' + str(closest_capsule_coordinates[0]) + ',' + str(closest_capsule_coordinates[1])\n",
    "                    if key not in greedy_moves:\n",
    "                        find_shortest_path(current_position, closest_capsule_coordinates, list_of_cells, dict_of_cells)\n",
    "                    action = greedy_moves[key]\n",
    "                    return action\n",
    "                else:\n",
    "                    subnature = 'GetFood'\n",
    "                 \n",
    "            if subnature == 'GetEdibleGhost':\n",
    "                closest_edible_ghost_coordinates = self.find_closest_edible_ghost(state)[0]\n",
    "                if closest_edible_ghost_coordinates != None:\n",
    "                    current_position = list(state.getPacmanPosition())\n",
    "                    key = str(current_position[0]) + ',' + str(current_position[1]) + '-' + str(closest_edible_ghost_coordinates[0]) + ',' + str(closest_edible_ghost_coordinates[1])\n",
    "                    if key not in greedy_moves:\n",
    "                        find_shortest_path(current_position, closest_edible_ghost_coordinates, list_of_cells, dict_of_cells)\n",
    "                    action = greedy_moves[key]\n",
    "                    return action\n",
    "                else:\n",
    "                    subnature = 'GetFood'\n",
    "                \n",
    "            if subnature == 'GetFood':\n",
    "                closest_food_coordinates = self.find_closest_food(state)[0]\n",
    "                current_position = list(state.getPacmanPosition())\n",
    "                key = str(current_position[0]) + ',' + str(current_position[1]) + '-' + str(closest_food_coordinates[0]) + ',' + str(closest_food_coordinates[1])\n",
    "                if key not in greedy_moves:\n",
    "                    find_shortest_path(current_position, closest_food_coordinates, list_of_cells, dict_of_cells)\n",
    "                action = greedy_moves[key]\n",
    "                return action\n",
    "            if subnature == 'EscapeFromGhost':\n",
    "                current_position = list(state.getPacmanPosition())\n",
    "                closest_ghost_coordinates = self.find_closest_active_ghost(state)[0]\n",
    "                if closest_ghost_coordinates != None:\n",
    "                    max_dist = 0\n",
    "                    max_dist_move = None\n",
    "                    for action in state.getLegalPacmanActions():\n",
    "                        if action == Directions.WEST:\n",
    "                            next_position = [current_position[0] - 1, current_position[1]]\n",
    "                        elif action == Directions.EAST:\n",
    "                            next_position = [current_position[0] + 1, current_position[1]]\n",
    "                        elif action == Directions.NORTH:\n",
    "                            next_position = [current_position[0], current_position[1] + 1]\n",
    "                        elif action == Directions.SOUTH:\n",
    "                            next_position = [current_position[0], current_position[1] - 1]\n",
    "                        else:\n",
    "                            next_position = [current_position[0], current_position[1]]\n",
    "                        if manhattanDistance(next_position, closest_ghost_coordinates) > max_dist:\n",
    "                            max_dist_move = action\n",
    "                            max_dist = manhattanDistance(next_position, closest_ghost_coordinates)\n",
    "                    action = max_dist_move\n",
    "                else:\n",
    "                    action = Directions.STOP\n",
    "                \n",
    "                return action\n",
    "    \n",
    "    # Função que retorna o movimento do Pacman com base no resultado do parsing.\n",
    "    def getAction(self, state):\n",
    "        action = self.parse_policy(self.policy, state)\n",
    "        return action\n",
    "    \n",
    "\n",
    "###################################################\n",
    "# Implementações relativas ao algoritmo genético. #\n",
    "###################################################\n",
    "\n",
    "# Variáveis globais\n",
    "args = agent, layout, list_of_cells, dict_of_cells, greedy_moves = None\n",
    "\n",
    "# Imprime uma árvore de decisão.\n",
    "def print_policy(root):\n",
    "    if root.value != None:\n",
    "        print(root.value)\n",
    "    elif root.subnature != None:\n",
    "        print(root.subnature)\n",
    "    else:\n",
    "        print(root.nature)\n",
    "    print()\n",
    "    for c in root.children:\n",
    "        print_policy(c)\n",
    "\n",
    "# Constrói uma árvore de decisão (política) de maneira aleatorizada.\n",
    "def build_random_policy(node, isRoot, max_level, max_const):\n",
    "    node.nature = 'Iflte'\n",
    "    if isRoot:\n",
    "        node.isRoot = True\n",
    "        \n",
    "    c1 = Node()\n",
    "    c2 = Node()\n",
    "    c3 = Node()\n",
    "    c4 = Node()\n",
    "    \n",
    "    for c in [c1,c2,c3,c4]:\n",
    "        c.parent = node\n",
    "\n",
    "    if node.isRoot:\n",
    "        c1.nature = 'Data'\n",
    "        c1.subnature = 'DistToActiveGhost'\n",
    "        c2.nature = 'Data'\n",
    "        c2.subnature = 'Constant'\n",
    "        c2.value = random.randint(1, max_const)\n",
    "        c3.nature = 'Action'\n",
    "        c3.subnature = 'EscapeFromGhost'\n",
    "        c4.nature = 'Iflte'\n",
    "        build_random_policy(c4, False, max_level-1, max_const)  \n",
    "    else:\n",
    "        c1.nature = 'Data'\n",
    "        c1.subnature = random.choice([item for item in data_subnatures if item != 'Constant'])\n",
    "        c2.nature = 'Data'  \n",
    "        c2.subnature = random.choice([item for item in data_subnatures if item != c1.subnature])\n",
    "        if c2.subnature == 'Constant':\n",
    "            c2.value = random.randint(1, max_const)\n",
    "        if max_level > 1:\n",
    "            c3.nature = random.choice(['Action', 'Iflte'])\n",
    "            c4.nature = random.choice(['Action', 'Iflte'])\n",
    "        else:\n",
    "            c3.nature = 'Action'\n",
    "            c4.nature = 'Action'\n",
    "\n",
    "        if c3.nature == 'Action':\n",
    "            c3.subnature = random.choice(action_subnatures)\n",
    "        elif c3.nature == 'Iflte':\n",
    "            build_random_policy(c3, False, max_level-1, max_const)\n",
    "\n",
    "        if c4.nature == 'Action':\n",
    "            c4.subnature = random.choice([item for item in action_subnatures if item != c3.subnature])\n",
    "        elif c4.nature == 'Iflte':\n",
    "            build_random_policy(c4, False, max_level-1, max_const)\n",
    "    \n",
    "    node.children = np.array([c1, c2, c3, c4])\n",
    "\n",
    "# Função que retorna uma população com indivíduos produzidos aleatoriamente\n",
    "def get_new_population(population_size, max_depth, max_const):\n",
    "    population = []\n",
    "\n",
    "    for i in range(population_size):\n",
    "        root = Node()\n",
    "        build_random_policy(root, False, max_depth, max_const)\n",
    "        population.append(root)\n",
    "\n",
    "    return np.array(population)\n",
    "\n",
    "# Função que calcula o fitness de uma política.\n",
    "def evaluate_policy(policy):\n",
    "    agent.policy = policy\n",
    "    win_counter = 0\n",
    "    fitness = 0\n",
    "    \n",
    "    for i in range(10):\n",
    "        games = runGames(**args)\n",
    "        fitness += games[0].state.getScore()\n",
    "        if games[0].state.isWin():\n",
    "            win_counter += 1\n",
    "            fitness += 10000\n",
    "    fitness = float(fitness)/10\n",
    "    \n",
    "    policy.fitness = fitness\n",
    "    policy.win_counter = win_counter\n",
    "\n",
    "# Função que retorna os nós de natureza Iflte de uma política (exceto a raiz).\n",
    "def get_iflte_nodes(policy):\n",
    "    root = policy\n",
    "    queue = [root]\n",
    "    iflte_nodes = []\n",
    "    while len(queue) > 0:\n",
    "        current_node = queue.pop(0)\n",
    "        if current_node.nature == 'Iflte':\n",
    "            if not current_node.isRoot:\n",
    "                iflte_nodes.append(current_node)\n",
    "            for c in current_node.children:\n",
    "                queue.append(c)\n",
    "    return np.array(iflte_nodes)\n",
    "\n",
    "# Função que retorna os nós de natureza não Iflte de uma política.\n",
    "def get_non_iflte_nodes(policy):\n",
    "    root = policy\n",
    "    queue = [root]\n",
    "    non_iflte_nodes = []\n",
    "    while len(queue) > 0:\n",
    "        current_node = queue.pop(0)\n",
    "        if current_node.nature != 'Iflte':\n",
    "            non_iflte_nodes.append(current_node)\n",
    "        for c in current_node.children:\n",
    "            queue.append(c)\n",
    "    return np.array(non_iflte_nodes)\n",
    "\n",
    "# Função que substitui a população.\n",
    "def replace_population(population, new_population_size): \n",
    "    population = sorted(population, key=lambda x:x.fitness, reverse=True)\n",
    "    new_population = []\n",
    "    for i in range(new_population_size):\n",
    "        new_population.append(population[i])\n",
    "    \n",
    "    return np.array(new_population)\n",
    "\n",
    "# Função que faz o crossover em uma geração.\n",
    "def make_crossover(population, n_crossover):\n",
    "    \n",
    "    new_policys = []\n",
    "    for i in range(n_crossover):\n",
    "        p1 = copy.deepcopy(random.choice(population))\n",
    "        p2 = copy.deepcopy(random.choice(population))\n",
    "        \n",
    "        p1_iflte_nodes = get_iflte_nodes(p1)\n",
    "        p2_iflte_nodes = get_iflte_nodes(p2)\n",
    "    \n",
    "        p1_selected_iflte_node = random.choice(p1_iflte_nodes)\n",
    "        p2_selected_iflte_node = random.choice(p2_iflte_nodes)\n",
    "        \n",
    "        if p1_selected_iflte_node.parent != None:\n",
    "            if p1_selected_iflte_node.parent.children[2] == p1_selected_iflte_node:\n",
    "                p1_selected_iflte_node.parent.children[2] = p2_selected_iflte_node\n",
    "            else:\n",
    "                p1_selected_iflte_node.parent.children[3] = p2_selected_iflte_node\n",
    "        \n",
    "        if p2_selected_iflte_node.parent != None:\n",
    "            if p2_selected_iflte_node.parent.children[2] == p2_selected_iflte_node:\n",
    "                p2_selected_iflte_node.parent.children[2] = p1_selected_iflte_node\n",
    "            else:\n",
    "                p2_selected_iflte_node.parent.children[3] = p1_selected_iflte_node\n",
    "\n",
    "        tmp = p1_selected_iflte_node.parent\n",
    "        p1_selected_iflte_node.parent = p2_selected_iflte_node.parent\n",
    "        p2_selected_iflte_node.parent = p1_selected_iflte_node.parent\n",
    "        \n",
    "        new_policys.append(p1)\n",
    "        new_policys.append(p2)\n",
    "        \n",
    "    for p in new_policys:\n",
    "        evaluate_policy(p)\n",
    "        \n",
    "    return np.array(new_policys)\n",
    "\n",
    "# Função que faz o processo de mutação de uma geração.\n",
    "def make_mutation(population, n_mutations, max_const):\n",
    "    new_policys = []\n",
    "    for i in range(n_mutations):\n",
    "        p = copy.deepcopy(random.choice(population))\n",
    "        p_non_iflte_nodes = get_non_iflte_nodes(p)\n",
    "        p_seleted_node = random.choice(p_non_iflte_nodes)\n",
    "        \n",
    "        if p_seleted_node.nature == 'Action':\n",
    "            p_seleted_node.subnature = random.choice([item for item in action_subnatures if item != p_seleted_node.subnature])\n",
    "        elif p_seleted_node.nature == 'Data':\n",
    "            p_seleted_node.subnature = random.choice([item for item in data_subnatures if item != p_seleted_node.subnature])\n",
    "            if p_seleted_node.subnature == 'Constant':\n",
    "                p_seleted_node.value = random.randint(1, max_const)\n",
    "        \n",
    "        new_policys.append(p)\n",
    "    \n",
    "    for p in new_policys:\n",
    "        evaluate_policy(p)\n",
    "    \n",
    "    return np.array(new_policys)\n",
    "\n",
    "# Função que implementa o algoritmo genético\n",
    "def genetic_algorithm(n_iterations, population_size, max_depth, max_const, n_children, n_mutates):\n",
    "    population = get_new_population(population_size, max_depth, max_const)\n",
    "    \n",
    "    for p in population:\n",
    "        evaluate_policy(p)\n",
    "    '''\n",
    "    j = 1\n",
    "    for p in population:\n",
    "        print(\"#\" + str(j))\n",
    "        print_policy(p)\n",
    "        print()\n",
    "        j += 1\n",
    "    '''\n",
    "        \n",
    "    for i in range(n_iterations):\n",
    "        print(\"Generation #\" + str(i+1))\n",
    "        \n",
    "        min_win = math.inf\n",
    "        max_win = -math.inf\n",
    "        avg_win = 0\n",
    "        min_fitness = math.inf\n",
    "        max_fitness = -math.inf\n",
    "        avg_fitness = 0\n",
    "        for p in population:\n",
    "            min_win = min(min_win, p.win_counter)\n",
    "            max_win = max(max_win, p.win_counter)\n",
    "            avg_win += p.win_counter\n",
    "            min_fitness = min(min_fitness, p.fitness)\n",
    "            max_fitness = max(max_fitness, p.fitness)\n",
    "            avg_fitness += p.fitness\n",
    "        avg_win = float(avg_win)/len(population)\n",
    "        avg_fitness = float(avg_fitness)/len(population)\n",
    "        \n",
    "        print(\"Min Win:\", min_win)\n",
    "        print(\"Max Win:\", max_win)\n",
    "        print(\"Avg Win:\", avg_win)\n",
    "        print(\"Min Fitness:\", min_fitness)\n",
    "        print(\"Max Fitness:\", max_fitness)\n",
    "        print(\"Avg Fitness:\", avg_fitness)\n",
    "        print()\n",
    "        \n",
    "        if i+1 < n_iterations:\n",
    "            new_chromossomes_cross = make_crossover(population, int(n_children/2))\n",
    "            new_chromossomes_mutation = make_mutation(population, n_mutations, max_const)            \n",
    "            population = replace_population(np.concatenate((population, new_chromossomes_cross, new_chromossomes_mutation)), population_size)\n",
    "\n",
    "# Função que inicializa o layout do labirinto\n",
    "def initialize_layout(layout_name):\n",
    "    input_str = \"-q -l \" + layout_name\n",
    "    args = readCommand(input_str.split())\n",
    "    agent = GpAgent()\n",
    "    args['pacman'] = agent\n",
    "    layout = args['layout']\n",
    "    list_of_cells, dict_of_cells = get_list_and_dict_of_cells(layout)\n",
    "    greedy_moves = dict([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-fleece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 36516285968929\n",
    "random.seed(seed)\n",
    "\n",
    "initialize_layout('smallClassic')\n",
    "n_generations = 200\n",
    "population_size = 10\n",
    "tree_max_depth = 5\n",
    "max_constant = 5\n",
    "n_offspring = 4\n",
    "n_mutations = 2\n",
    "genetic_algorithm(n_generations, population_size, tree_max_depth, max_constant, n_offspring, n_mutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_layout('mediumClassic')\n",
    "n_generations = 200\n",
    "population_size = 10\n",
    "tree_max_depth = 5\n",
    "max_constant = 5\n",
    "n_offspring = 4\n",
    "n_mutations = 2\n",
    "genetic_algorithm(n_generations, population_size, tree_max_depth, max_constant, n_offspring, n_mutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_layout('originalClassic')\n",
    "n_generations = 200\n",
    "population_size = 10\n",
    "tree_max_depth = 5\n",
    "max_constant = 5\n",
    "n_offspring = 4\n",
    "n_mutations = 2\n",
    "genetic_algorithm(n_generations, population_size, tree_max_depth, max_constant, n_offspring, n_mutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-february",
   "metadata": {},
   "source": [
    "# 2.3 Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-offset",
   "metadata": {},
   "source": [
    "# 3 Parte II - Aprendizado por Reforço"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-frank",
   "metadata": {},
   "source": [
    "# 4 Comparação entre modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-explorer",
   "metadata": {},
   "source": [
    "# 5 *Considerações Finais*\n",
    "\n",
    "Neste *notebook*, apresentamos as definições dos problemas explorados, os métodos utilizados para resolvê-los, assim como os resultados obtidos, de acordo com a especificação do projeto. Em particular, destacamos os seguintes pontos que foram exibidos no relatório.\n",
    "\n",
    "Na Seção 2..\n",
    "\n",
    "* A\n",
    "* B\n",
    "* C\n",
    "\n",
    "Portanto, cumprimos todas as tarefas previstas na especificação do projeto relativa à Parte $I$.\n",
    "\n",
    "Na Seção 3 ...\n",
    "\n",
    "* A\n",
    "* B\n",
    "* C\n",
    " \n",
    "Assim, cumprimos todas as tarefas previstas na especificação do projeto relativa à Parte $II$.\n",
    "\n",
    "Além disso, Na Seção 4, discutimos:\n",
    "\n",
    "* A\n",
    "* B\n",
    "* C\n",
    "\n",
    "Ambos discentes participaram ativamente durante todo o projeto. O aluno Felipe focou os seus esforços na Parte $I$ do projeto, enquanto a aluna Elisa esteve mais envolvida com a Parte $II$. Contudo, as pesquisas, revisões de código-fonte e tomadas de decisão foram sempre feitas em conjunto. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-school",
   "metadata": {},
   "source": [
    "# 6 Referências Bibliográficas\n",
    "\n",
    "[1] ALPAYDIN, E. Introduction to Machine Learning. MIT Press, 3rd edition, 2014.\n",
    "\n",
    "[2] BISHOP, C. M. Pattern Recognition and Machine Learning. Springer. Cambridge, 2007.\n",
    "\n",
    "[3] MARSLAND, S. Machine Learning: an algorithm perspective. CRC Press, 2nd edition, 2015.\n",
    "\n",
    "[4] BRANDSTETTER, Matthias F.; AHMADI, Samad. Reactive control of Ms. Pac Man using information retrieval based on genetic programming. In: 2012 IEEE Conference on Computational Intelligence and Games (CIG). IEEE, 2012. p. 250-256.\n",
    "\n",
    "[5] ALHEJALI, Atif M.; LUCAS, Simon M. Evolving diverse Ms. Pac-Man playing agents using genetic programming. In: 2010 UK Workshop on Computational Intelligence (UKCI). IEEE, 2010. p. 1-6.\n",
    "\n",
    "[6] GNANASEKARAN, Abeynaya; FABA, Jordi Feliu; AN, Jing. Reinforcement Learning in Pacman., 2017.\n",
    "\n",
    "[7] The Pac-Man Projects. Disponível em https://cs.brynmawr.edu/Courses/cs372/fall2017/ (acesso em junho de 2021)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
